---
title: "Text classification"
author: "Jae Yeon Kim"
output:
html_document: 
  toc: true
  theme: united
---

Resources 
- https://www.hvitfeldt.me/blog/text-classification-with-tidymodels/#introduction
- https://juliasilge.com/blog/tidy-text-classification/
- http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/
- https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/
- https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c
- https://www.hvitfeldt.me/blog/authorship-classification-with-tidymodels-and-textrecipes/
- https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html
- https://stackoverflow.com/questions/43744399/can-i-get-classification-accuracy-and-cohens-kappa-from-glm-results
- http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/
- http://fisher.stats.uwo.ca/faculty/aim/2019/9850/RNotebooks/21_CaravanRevisited_Apr7.html
- https://fderyckel.github.io/2016-12-07-Texts_Classification_in_R/
- https://cfss.uchicago.edu/notes/supervised-text-classification/

## 0. Setup 

I tweaked the global option of the R Markdown to enlarge figures produced by ggplot2.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, 
                      echo = FALSE, warning = FALSE, message = FALSE) # global setting for enlarging image size
```

```{r}

# Clean up the environment

# rm(list = ls())

# Import libraries (adapted from this link: https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        tidyverse, # for the tidyverse framework
        tidymodels, # for tidy modeling  
        caret, # for ML
        pROC, # for ROC
        parsnip, # for modeling
        tidytext, # for text analysis in a tidy way
        lubridate, # for date manipulation
        textrecipes, # for preprocessing text data
        text2vec, # for preprocessing text data
        stopwords, # for stopwords
        tokenizers, # for tokenizing
        textclean, # for cleaning text
        hunspell, # for cleaning text 
        textshape, # for cleaning text 
        tm, # for cleaning text 
        tidyr, # for creating tidy data
        furrr, # for applying mapping functions
        ggpubr, # for arranging ggplot2 
        ggthemes, # for fancy ggplot themes
        glmnet, # for logistic regression classifier
        randomForest, # for random forest classifier
        rpart, # for recursive partitioning
        caTools, # for ML
        splitstackshape, # for stratified random sampling
        DMwR, # for daealing with imbalanced classes 
        imbalance, # for dealing wiht imbalanced classes 
        yardstick # for ML measures
)

```

https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/

## 1. Importing files 


```{r}

# Labeled data 

labeled_asian <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_asian.csv")[,-2] 
labeled_black <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_black.csv")[,-2]

# Unlabeled data 

unlabeled_asian <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_asian.csv")[,-2]
unlabeled_black <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_black.csv")[,-2]

# rename the missing col name 

colnames(labeled_asian)[1] <- "id"
colnames(labeled_black)[1] <- "id"
colnames(unlabeled_asian)[1] <- "id"
colnames(unlabeled_black)[1] <- "id"

# correlation and kappa 

irr_summary <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/outputs/irr_summary.csv")
```

## 2. Cleaning text

### 2.1. Creating my custom text clean function 

```{r}

clean_text <- function(document){
  document %>% 
    tolower() %>% # low key 
    gsub("[\r?\n]", "", .) %>% # remove \n
    gsub("[\"]", "", .) %>%
    gsub("[.]", "", .) %>%
    gsub("[,]", "", .) %>%
    add_comma_space() %>%
    str_trim() # remove whitespace 
}

```

### 2.2 Applying the function to each data 

```{r}

labeled_asian$text <- clean_text(labeled_asian$text) 
labeled_black$text <- clean_text(labeled_black$text) 

labeled_asiank$text[1]
```

## 3. Feature engineering 

```{r}

train_test_response <- function(dataset, measure){
  
data_counts <- map_df(1:2,
                      ~ unnest_tokens(dataset, word, text, 
                                      token = "ngrams", n = .x)) %>%
  anti_join(stop_words, by = "word") %>%
  count(id, word, sort = TRUE)

words_10 <- data_counts %>%
  group_by(word) %>%
  summarise(n = n()) %>% 
  filter(n >= 10) %>%
  select(word)

data_dtm <- data_counts %>%
  right_join(words_10, by = "word") %>%
  bind_tf_idf(word, id, n) %>%
  cast_dtm(id, word, tf_idf)

meta <- tibble(id = as.numeric(dimnames(data_dtm)[[1]])) %>%
  left_join(dataset[!duplicated(dataset$id), ], by = "id")

trainIndex <- createDataPartition(meta[['year']], p = 0.8, list = FALSE, times = 1)

#perform grouped K means  

data_df_train <- data_dtm[trainIndex, ] %>% as.matrix() %>% as.data.frame()

data_df_test <- data_dtm[-trainIndex, ] %>% as.matrix() %>% as.data.frame()

response_train <- meta[[measure]][trainIndex]

ground_truth <- factor(meta[[measure]][-trainIndex])

result <- list(data_df_train, data_df_test, response_train, ground_truth)

return(result)

}

asian_lp <- train_test_response(labeled_asian, 'linked_progress')
asian_lh <- train_test_response(labeled_asian, 'linked_hurt')
black_lp <- train_test_response(labeled_black, 'linked_progress')
black_lh <- train_test_response(labeled_black, 'linked_hurt')

```

## 3. Model building and testing  

### 3.1. Creating a function for model building and testing 

```{r}

build_test <- function(dataset){
  
data_df_train <- dataset[[1]] 
data_df_test <- dataset[[2]]
response_train <- dataset[[3]]
ground_truth <- dataset[[4]]

trctrl <- trainControl(method = "cv",
                      number = 10)

nb_mod <- train(x = data_df_train,
                y = as.factor(response_train),
                method = "naive_bayes",
                trControl = trctrl,
                tuneGrid = data.frame(laplace = 0,
                                      usekernel = FALSE,
                                      adjust = FALSE))
nb_pred <- predict(nb_mod,
                   newdata = data_df_test)

nb_cm <- confusionMatrix(nb_pred, ground_truth)

logitboost_mod <- train(x = data_df_train,
                        y = as.factor(response_train),
                        method = "LogitBoost",
                        trControl = trctrl)

logitboost_pred <- predict(logitboost_mod,
                           newdata = data_df_test)

logitboost_cm <- confusionMatrix(logitboost_pred, ground_truth)

rf_mod <- train(x = data_df_train, 
                y = as.factor(response_train), 
                method = "ranger",
                trControl = trctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(data_df_train)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))
rf_pred <- predict(rf_mod,
                   newdata = data_df_test)

rf_cm <- confusionMatrix(rf_pred, ground_truth)

acu_results <- rbind(
  nb_cm$overall,
  logitboost_cm$overall,
  rf_cm$overall
  ) %>%
  as.data.frame() %>%
  mutate(model = c("Naive-Bayes", "LogitBoost", "Random forest"))

balanced_acu_results <- c(
  bal_accuracy_vec(nb_pred, ground_truth),
  bal_accuracy_vec(logitboost_pred, ground_truth),
  bal_accuracy_vec(rf_pred, ground_truth))
  
mod_results <- data.frame(acu = acu_results$Accuracy,
                          kappa = acu_results$Kappa,
                          balanced_acu = balanced_acu_results) %>%
  
  mutate(model = c("Naive-Bayes", "LogitBoost", "Random forest")) 

return(mod_results)}

```

```{r}

all_model <- bind_rows(mutate(build_test(asian_lp), group = "Asian_Americans", type = "Linked_progress"),
          mutate(build_test(asian_lh), group = "Asian_Americans", type = "Linked_hurt"),
          mutate(build_test(black_lp), group = "African_Americans", type = "Linked_progress"),
          mutate(build_test(black_lh), group = "African_Americans", type = "Linked_hurt"))

numerify <- function(data){
  data %>% unlist() %>% as.numeric()
}

irr_resummary <- data.frame(content_kappa = c(numerify(irr_summary[1:2, 2]), numerify(irr_summary[1:2, 3])),
                            content_agreement = c(numerify(irr_summary[1:2, 4]), numerify(irr_summary[1:2, 5])),
                            type = c("Linked_progress", "Linked_hurt", "Linked_progress", "Linked_hurt"),
                            group = c("Asian_Americans", "Asian_Americans", "African_Americans", "African_Americans")
)

merged_model <- all_model %>% right_join(irr_resummary) 

accu_plot <- merged_model %>%
  gather(metrices, performance, acu, balanced_acu) %>%
  ggplot(aes(x = fct_reorder(model, performance), y = performance, fill = metrices)) +
    geom_col(position = "dodge") +
    geom_hline(aes(yintercept = content_agreement), linetype = "dashed") +
    facet_grid(group~type) +
    theme_base() +
    labs(title = "Accuracy", x = "Model", y = "Performance (%)") +
    coord_flip()

kappa_plot <- merged_model %>%
  ggplot(aes(x = fct_reorder(model, kappa), y = kappa)) +
    geom_col(position = "dodge") +
    geom_hline(aes(yintercept = content_kappa), linetype = "dashed") +
    facet_grid(group~type) +
    theme_base() +
    labs(title = "Reliability", x = "Model", y = "Performance (%)") +
    coord_flip()

ggarrange(accu_plot, kappa_plot, ncol = 1, nrow = 2)
```

## 4. Predictions 

```{r}

#function(text, model){
  
 #  features = text %>% cleaned_
    
#   predictions = model.predict(features)
    
 # return(preds)
#}

```

```{r}
# asian
## unlabeled
#asian_lp_full = test_text(asian_unlabeled['text'], asian_lp)
#asian_lh_full = test_text(asian_unlabeled['text'], asian_lh)

# black
## unlabeled 
#black_lp_full = test_text(black_unlabeled['text'], black_lp)
#black_lh_full = test_text(black_unlabeled['text'], black_lh)
```


## 5. Exporting results as a csv file 
