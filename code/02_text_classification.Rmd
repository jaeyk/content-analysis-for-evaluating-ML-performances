---
title: "Text classification"
author: "Jae Yeon Kim"
output:
html_document: 
  toc: true
  theme: united
---

Resources 
- https://www.hvitfeldt.me/blog/text-classification-with-tidymodels/#introduction
- https://juliasilge.com/blog/tidy-text-classification/
- http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/
- https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/
- https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c
- https://www.hvitfeldt.me/blog/authorship-classification-with-tidymodels-and-textrecipes/
- https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html
- https://stackoverflow.com/questions/43744399/can-i-get-classification-accuracy-and-cohens-kappa-from-glm-results
- http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/
- http://fisher.stats.uwo.ca/faculty/aim/2019/9850/RNotebooks/21_CaravanRevisited_Apr7.html
- https://fderyckel.github.io/2016-12-07-Texts_Classification_in_R/
- https://cfss.uchicago.edu/notes/supervised-text-classification/
- https://abndistro.com/post/2019/02/17/text-classification-using-text2vec/

## 0. Setup 

I tweaked the global option of the R Markdown to enlarge figures produced by ggplot2.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, 
                      echo = FALSE, warning = FALSE, message = FALSE) # global setting for enlarging image size
```

```{r}

# Clean up the environment

rm(list = ls())

# Import libraries (adapted from this link: https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        tidyverse, # for the tidyverse framework
        tidymodels, # for tidy modeling  
        caret, # for ML
        pROC, # for ROC
        parsnip, # for modeling
        tidytext, # for text analysis in a tidy way
        lubridate, # for date manipulation
        textrecipes, # for preprocessing text data
        text2vec, # for preprocessing text data
        stopwords, # for stopwords
        tokenizers, # for tokenizing
        textclean, # for cleaning text
        hunspell, # for cleaning text 
        textshape, # for cleaning text
        SuperLearner, # for ensemble models 
        tm, # for cleaning text 
        tidyr, # for creating tidy data
        furrr, # for applying mapping functions
        ggpubr, # for arranging ggplot2 
        ggthemes, # for fancy ggplot themes
        glmnet, # for logistic regression classifier
        randomForest, # for random forest classifier
        rpart, # for recursive partitioning
        caTools, # for ML
        splitstackshape, # for stratified random sampling
        DMwR, # for daealing with imbalanced classes 
        imbalance, # for dealing wiht imbalanced classes 
        yardstick, # for ML measures
        superml # for building ML like using scikit-learn in R
)

```

https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/

## 1. Importing files 


```{r}

# Labeled data 

labeled_asian <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_asian.csv")[,-2] 
labeled_black <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_black.csv")[,-2]

# Unlabeled data 

unlabeled_asian <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_asian.csv")[,-2]
unlabeled_black <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_black.csv")[,-2]

# rename the missing col name 

colnames(labeled_asian)[1] <- "id"
colnames(labeled_black)[1] <- "id"
colnames(unlabeled_asian)[1] <- "id"
colnames(unlabeled_black)[1] <- "id"

# correlation and kappa 

irr_summary <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/outputs/irr_summary.csv")

```

## 2. Cleaning text

### 2.1. Creating my custom text clean function 

```{r}

clean_text <- function(document){
    document %>% 
    tolower() %>% # low key 
    gsub("[\r?\n]", "", .) %>% # remove \n
    gsub("[\"]", "", .) %>%
    gsub("[.]", "", .) %>%
    gsub("[,]", "", .) %>%
    gsub("[-]", "", .) %>%
    gsub("[']", "", .) %>%
    str_trim() # remove whitespace 
}

```

### 2.2. Applying the function to each data 

```{r}
  
labeled_asian$text <- clean_text(labeled_asian$text) 
labeled_black$text <- clean_text(labeled_black$text) 

```

## 3. Feature engineering 

```{r}

train_test_response <- function(dataset, measure){
  
data_counts <- unnest_tokens(dataset, word, text) %>%
  count(id, word, sort = TRUE)

words_10 <- data_counts %>%
  group_by(word) %>%
  summarise(n = n()) %>% 
  filter(n >= 10) %>%
  select(word)

data_dtm <- data_counts %>%
  right_join(words_10, by = "word") %>%
  bind_tf_idf(word, id, n) %>%
  cast_dtm(id, word, tf_idf)

meta <- tibble(id = as.numeric(dimnames(data_dtm)[[1]])) %>%
  left_join(dataset[!duplicated(dataset$id), ], by = "id")

trainIndex <- createDataPartition(meta[[measure]], p = 0.8, list = FALSE, times = 1) # stratified by response variable 
#perform grouped K means  

data_df_train <- data_dtm[trainIndex, ] %>% as.matrix() %>% as.data.frame()

data_df_test <- data_dtm[-trainIndex, ] %>% as.matrix() %>% as.data.frame()

response_train <- meta[[measure]][trainIndex]

ground_truth <- factor(meta[[measure]][-trainIndex])

result <- list(data_df_train, data_df_test, response_train, ground_truth)

return(result)

}

asian_lp <- train_test_response(labeled_asian, 'linked_progress')
asian_lh <- train_test_response(labeled_asian, 'linked_hurt')
black_lp <- train_test_response(labeled_black, 'linked_progress')
black_lh <- train_test_response(labeled_black, 'linked_hurt')

```

## 3. Model building and testing  

### 3.1. Creating a function for model building and testing 

```{r}

build_test <- function(dataset){
  
################## DATA SETUP #######################
  
data_df_train <- dataset[[1]] 
data_df_test <- dataset[[2]]
response_train <- dataset[[3]]
ground_truth <- dataset[[4]]

################# TRAIN CONTROL SETUP ##################

set.seed(1234)

trctrl <- trainControl(method = "cv", # cross validation 
                      number = 10) # repeat 10 times 

################# NAIVE BAYES ####################

nb_mod <- train(x = data_df_train,
                y = as.factor(response_train),
                method = "naive_bayes",
                trControl = trctrl,
                tuneGrid = data.frame(laplace = 0,
                                      usekernel = FALSE,
                                      adjust = FALSE))

nb_pred <- predict(nb_mod,
                   newdata = data_df_test)

nb_cm <- confusionMatrix(nb_pred, ground_truth)

################# PENALIZED LOGIT ####################

cv <- cv.glmnet(x = as.matrix(data_df_train),
                y = as.factor(response_train), alpha = 1, family = "binomial")

lasso_mod <- train(x = data_df_train,
                  y = as.factor(response_train), 
                  method = "glmnet",
                  trControl = trctrl,
                  tuneGrid = expand.grid(alpha = 1,
                                       lambda = cv$lambda.min))

lasso_pred <- predict(lasso_mod,
                           newdata = data_df_test)
  
lasso_cm <- confusionMatrix(lasso_pred, ground_truth)

################# LOGIT BOOST ####################

logitboost_mod <- train(x = data_df_train,
                        y = as.factor(response_train),
                        method = "LogitBoost",
                        trControl = trctrl)

logitboost_pred <- predict(logitboost_mod,
                           newdata = data_df_test)

logitboost_cm <- confusionMatrix(logitboost_pred, ground_truth)

################# RANDOM FOREST ####################

rf_mod <- train(x = data_df_train, 
                y = as.factor(response_train), 
                method = "ranger",
                trControl = trctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(data_df_train)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))

rf_pred <- predict(rf_mod,
                   newdata = data_df_test)

rf_cm <- confusionMatrix(rf_pred, ground_truth)

################# STACKED MODEL ####################

################# BINDING RESULTS ####################

# Accuracy 
acu_results <- rbind(
  nb_cm$overall,
  lasso_cm$overall,
  logitboost_cm$overall,
  rf_cm$overall
  ) %>%
  as.data.frame() %>%
  mutate(model = c("Naive-Bayes", "Penalized logit", "LogitBoost", "Random forest"))

# Balanced accuracy 

balanced_acu_results <- c(
  bal_accuracy_vec(nb_pred, ground_truth),
  bal_accuracy_vec(lasso_pred, ground_truth),
  bal_accuracy_vec(logitboost_pred, ground_truth),
  bal_accuracy_vec(rf_pred, ground_truth))

# Putting all together as a dataframe

mods <- list(nb_mod, lasso_mod, logitboost_mod, rf_mod)
  
metrices <- data.frame(acu = acu_results$Accuracy,
                          kappa = acu_results$Kappa,
                          balanced_acu = balanced_acu_results) %>%
  mutate(model = c("Naive-Bayes", "Penalized logit", "LogitBoost", "Random forest")) 

mod_results <- list(mods, metrices)

return(mod_results)}

```

### 3.2. Extracting and visualizing the model outcomes

```{r}

# Using multiple cores 
doMC::registerDoMC(cores = 4)

asian_lp_model <- build_test(asian_lp)
asian_lh_model <- build_test(asian_lh)
black_lp_model <- build_test(black_lp)
black_lh_model <- build_test(black_lh)

asian_lp_metrices <- asian_lp_model[2] %>% as.data.frame()
asian_lh_metrices <- asian_lh_model[2] %>% as.data.frame()
black_lp_metrices <- black_lp_model[2] %>% as.data.frame()
black_lh_metrices <- black_lh_model[2] %>% as.data.frame()

```

```{r}

all_model <- bind_rows(mutate(asian_lp_metrices, group = "Asian_Americans", type = "Linked_progress"),
          mutate(asian_lh_metrices, group = "Asian_Americans", type = "Linked_hurt"),
          mutate(black_lp_metrices, group = "African_Americans", type = "Linked_progress"),
          mutate(black_lh_metrices, group = "African_Americans", type = "Linked_hurt"))

numerify <- function(data){
  data %>% unlist() %>% as.numeric()
}

irr_resummary <- data.frame(content_kappa = c(numerify(irr_summary[1:2, 2]), numerify(irr_summary[1:2, 3])),
                            content_agreement = c(numerify(irr_summary[1:2, 4]), numerify(irr_summary[1:2, 5])),
                            type = c("Linked_progress", "Linked_hurt", "Linked_progress", "Linked_hurt"),
                            group = c("Asian_Americans", "Asian_Americans", "African_Americans", "African_Americans")
)

merged_model <- all_model %>% right_join(irr_resummary) 

```

```{r}

accu_plot <- merged_model %>%
  gather(metrices, performance, acu, balanced_acu) %>%
  ggplot(aes(x = fct_reorder(model, performance), y = performance, fill = metrices)) +
    geom_col(position = "dodge") +
    geom_hline(aes(yintercept = content_agreement), linetype = "dashed") +
    facet_grid(type~group) +
    theme_base() +
    labs(title = "Accuracy", x = "Model", y = "Performance (%)") +
    coord_flip()

kappa_plot <- merged_model %>%
  ggplot(aes(x = fct_reorder(model, kappa), y = kappa)) +
    geom_col(position = "dodge") +
    geom_hline(aes(yintercept = content_kappa), linetype = "dashed") +
    facet_grid(type~group) +
    theme_base() +
    labs(title = "Reliability", x = "Model", y = "Performance (%)") +
    coord_flip()

ggarrange(accu_plot, kappa_plot, ncol = 1, nrow = 2)

ggsave("/home/jae/content-analysis-for-evaluating-ML-performances/outputs/ml_performance.png")

```

## 4. Applying to the unlabeled data

### 4.1. Buidling a prediction function 

```{r}

extract_feature <- function(dataset){
 
data_counts <- unnest_tokens(dataset, word, text) %>%
  count(id, word, sort = TRUE) 

print("Data count works.")

words_10 <- data_counts %>%
  group_by(word) %>%
  summarise(n = n()) %>% 
  filter(n >= 10) %>%
  select(word)

print("Word 20 is working fine, too.")

data_dtm <- data_counts %>%
  right_join(words_10, by = "word") %>%
  bind_tf_idf(word, id, n) %>%
  cast_dtm(id, word, tf_idf)

print("Matrix transformation is complete.")

data_df_test <- data_dtm %>% as.matrix() %>% as.data.frame()
  
return(data_df_test)
}

```

```{r}

unlabeled_asian$text <- clean_text(unlabeled_asian$text)
unlabeled_black$text <- clean_text(unlabeled_black$text)

asian_dtm <- extract_feature(unlabeled_asian)
black_dtm <- extract_feature(unlabeled_black)

asian_lp_preds <- predict(asian_lp_model[[1]][[3]], asian_dtm)

asian_lh_preds <- predict(asian_lh_model[[1]][[3]], asian_dtm)

black_lp_preds <- predict(black_lp_model[[1]][[3]], black_dtm)

black_lh_preds <- predict(black_lh_model[[1]][[3]], black_dtm)

```

## 5. Exporting results as a csv file 

```{r}

```

