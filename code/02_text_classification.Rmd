---
title: "Text classification"
author: "Jae Yeon Kim"
output:
html_document: 
  toc: true
  theme: united
---

Resources 
- https://www.hvitfeldt.me/blog/text-classification-with-tidymodels/#introduction
- https://juliasilge.com/blog/tidy-text-classification/
- http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/
- https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/
- https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c
- https://www.hvitfeldt.me/blog/authorship-classification-with-tidymodels-and-textrecipes/
- https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html
- https://stackoverflow.com/questions/43744399/can-i-get-classification-accuracy-and-cohens-kappa-from-glm-results
- http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/
- http://fisher.stats.uwo.ca/faculty/aim/2019/9850/RNotebooks/21_CaravanRevisited_Apr7.html
- https://fderyckel.github.io/2016-12-07-Texts_Classification_in_R/
- https://cfss.uchicago.edu/notes/supervised-text-classification/
- https://abndistro.com/post/2019/02/17/text-classification-using-text2vec/
- https://www.kirenz.com/post/2019-09-16-r-text-mining/

## 0. Setup 

I tweaked the global option of the R Markdown to enlarge figures produced by ggplot2.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, 
                      echo = FALSE, warning = FALSE, message = FALSE) # global setting for enlarging image size
```

```{r}

# Clean up the environment

rm(list = ls())

# Import libraries (adapted from this link: https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        tidyverse, # for the tidyverse framework
        tidymodels, # for tidy modeling  
        caret, # for ML
        pROC, # for ROC
        parsnip, # for modeling
        tidytext, # for text analysis in a tidy way
        lubridate, # for date manipulation
        textrecipes, # for preprocessing text data
        text2vec, # for preprocessing text data
        stopwords, # for stopwords
        tokenizers, # for tokenizing
        textclean, # for cleaning text
        hunspell, # for cleaning text 
        textshape, # for cleaning text
        SuperLearner, # for ensemble models 
        tm, # for cleaning text 
        tidyr, # for creating tidy data
        furrr, # for applying mapping functions
        ggpubr, # for arranging ggplot2 
        ggthemes, # for fancy ggplot themes
        glmnet, # for logistic regression classifier
        randomForest, # for random forest classifier
        rpart, # for recursive partitioning
        caTools, # for ML
        splitstackshape, # for stratified random sampling
        DMwR, # for daealing with imbalanced classes 
        imbalance, # for dealing wiht imbalanced classes
        broom, # for visualizing coefficients
        yardstick, # for ML measures
        superml # for building ML like using scikit-learn in R
)

```

https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/

## 1. Importing files 

I skipped the first and second column (author) of all of these files because they are not informative. The original unlabeled data have linked progress and linked hurt columns (all NAs). I removed them too for the same reason.

```{r}

# Labeled data 

labeled_asian <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_asian.csv")[,-c(1,2)]
labeled_black <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_black.csv")[,-c(1,2)]

# Unlabeled data 

unlabeled_asian <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_asian.csv")[,-c(1,2,7,8)]
unlabeled_black <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_black.csv")[,-c(1,2,7,8)]

# From content analysis: Correlation coefficients and Kappa statistics 

irr_summary <- read_csv("/home/jae/content-analysis-for-evaluating-ML-performances/outputs/irr_summary.csv")[,-1]

```

## 2. Cleaning text

### 2.1. Creating my custom text clean function 

```{r}

clean_text <- function(document){
    document %>% 
    tolower() %>% # low key 
    # Some noisy patterns in the text 
    gsub("[\r?\n]", " ", .) %>% # remove \n
    gsub("[\"]", " ", .) %>%
    gsub("[.]", "", .) %>%
    gsub("[,]", " ", .) %>%
    gsub("[-]", "", .) %>%
    gsub("[']", "", .) %>%
    gsub("[/]", " ", .) %>%
    gsub("[/]", " ", .) %>%
    gsub("[(]", " ", .) %>%
    gsub("[)]", " ", .) %>%
    gsub("[;]", " ", .) %>%
    trimws() # remove whitespace 
}

```

### 2.2. Applying the function to each data 

```{r}
  
labeled_asian$text <- clean_text(labeled_asian$text) 
labeled_black$text <- clean_text(labeled_black$text) 

labeled_asian$text[5]
```

## 3. Feature engineering (bag of words)

### 3.1. Creating my custom function for feature engineering

```{r}

# dataset <- labeled_asian

vectorize_train_test <- function(dataset, measure){

# Add doc ID (this var turns out to be very important for sorting)

dataset <- dataset %>%
  mutate(doc_id = row_number())

# Define stopwords

stopword <- tibble::enframe(stopwords::stopwords("en")) 
stopword <- rename(stopword, word=value) 

# Tokenize and reduce noisy tokens (=stopwords)

data_counts <- unnest_tokens(dataset, word, text) %>%
  anti_join(stopword, by = 'word') %>% # remove stopwords 
  count(doc_id, word, sort = TRUE)

words_10 <- data_counts %>%
  group_by(word) %>%
  summarise(n = n()) %>% 
  filter(n >= 10) %>%
  dplyr::select(word)

# TF-IDF

data_dtm <- data_counts %>%
  right_join(words_10, by = "word") %>%
  bind_tf_idf(word, doc_id, n) %>%
  cast_dtm(doc_id, word, tf_idf)

# Meta 

meta <- tibble(doc_id = as.numeric(dimnames(data_dtm)[[1]])) %>%
  left_join(dataset[!duplicated(dataset$doc_id), ], by = "doc_id") %>% as.data.frame()

# Create two random samples (testing and training sets) 

set.seed(1234) # for reproducibility

# Stratifying by year variable
trainIndex <- createDataPartition(meta$year, p = 0.7, list = FALSE, times = 1) 

train_matrix <- data_dtm[trainIndex, ] %>% as.matrix() %>% as.data.frame() # Originally, it was a sparse matrix.
test_matrix <- data_dtm[-trainIndex, ] %>% as.matrix() %>% as.data.frame() # Originally, it was a sparse matrix.

train_response <- meta[trainIndex, measure] %>% as.factor() # Originally, it was matrix.
test_response <- meta[-trainIndex, measure] %>% as.factor() # Originally, it was matrix.

levels(train_response) <- c("No", "Yes") # Rename factor levels 
levels(test_response) <- c("No", "Yes") # Rename factor levels

result <- list(train_matrix, test_matrix, train_response, test_response)

return(result)

}

```

### 3.2. Applying the function to each data 

```{r}

asian_lp <- vectorize_train_test(labeled_asian, 'linked_progress')

asian_lh <- vectorize_train_test(labeled_asian, 'linked_hurt')

black_lp <- vectorize_train_test(labeled_black, 'linked_progress')

black_lh <- vectorize_train_test(labeled_black, 'linked_hurt')

```

## 3. Model building and testing  

### 3.1. Creating my custom function for model building and testing 

```{r}

# dataset <- asian_lp

build_test <- function(dataset){
    
################## DATA SETUP #######################
  
train_matrix <- dataset[[1]]

test_matrix <- dataset[[2]] 

train_response <- dataset[[3]] 

test_response <- dataset[[4]] 

print("Data setup is okay.")

################# TRAIN CONTROL SETUP ##################

# This setup defines measures of performance.

set.seed(1234)

trctrl <- trainControl(method="cv", 
                       number=10, 
                       sampling = "smote") 

print("Train control setup is okay.")

svm_mod <- train(x = train_matrix,
                 y = train_response,
                 method = "svmLinearWeights2",
                 trControl = trctrl,
                 tuneGrid = data.frame(cost = 1, 
                                       Loss = 0, 
                                       weight = 1))

svm_pred <- predict(svm_mod,
                    newdata = test_matrix)

svm_cm <- confusionMatrix(svm_pred, test_response)

print("SVM works.")

################# NAIVE BAYES ####################

nb_mod <- train(x = train_matrix,
                y = train_response,
                method = "naive_bayes",
                trControl = trctrl,
                tuneGrid = data.frame(laplace = 0,
                                usekernel = FALSE,
                                adjust = FALSE))

nb_pred <- predict(nb_mod,
                   newdata = test_matrix)

nb_cm <- confusionMatrix(nb_pred, test_response)

print("Naive Bayes model works.")

################# LOGIT BOOST ####################

logitboost_mod <- train(x = train_matrix,
                        y = train_response,
                        method = "LogitBoost",
                        trControl = trctrl)

logitboost_pred <- predict(logitboost_mod,
                           newdata = test_matrix)

logitboost_cm <- confusionMatrix(logitboost_pred, test_response)

print("Logit boost works.")

logitboost_cm

################# RANDOM FOREST ####################

rf_mod <- train(x = train_matrix, 
                y = train_response, 
                method = "ranger",
                trControl = trctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(train_matrix)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))

rf_pred <- predict(rf_mod,
                   newdata = test_matrix)

rf_cm <- confusionMatrix(rf_pred, test_response)

print("Random forest works.")

################# BINDING RESULTS ####################

# Accuracy 
acu_results <- rbind(
  svm_cm$overall,
  nb_cm$overall,
  xg_cm$overall,
  logitboost_cm$overall,
  rf_cm$overall
  ) %>%
  as.data.frame() %>%
  mutate(model = c("SVM", "Naive-Bayes", "XG Boost", "LogitBoost", "Random forest"))

# Balanced accuracy 

balanced_acu_results <- c(
  bal_accuracy_vec(svm_pred, test_response),
  bal_accuracy_vec(nb_pred, test_response),
  bal_accuracy_vec(logitboost_pred, test_response),
  bal_accuracy_vec(rf_pred, test_response))

# Putting all together as a dataframe

mods <- list(svm_mod, nb_mod, logitboost_mod, rf_mod)
  
metrices <- data.frame(acu = acu_results$Accuracy,
                       kappa = acu_results$Kappa,
                       balanced_acu = balanced_acu_results) %>%
  mutate(model = c("SVM", "Naive-Bayes", "LogitBoost", "Random forest")) 

mod_results <- list(mods, metrices)

return(mod_results)}

```

### 3.2. Extracting and visualizing the model outcomes

```{r}

##### Extract outcomes #####

asian_lp_model <- build_test(asian_lp)

asian_lh_model <- build_test(asian_lh)

black_lp_model <- build_test(black_lp)

black_lh_model <- build_test(black_lh)


##### Extract metrices #####

asian_lp_metrices <- asian_lp_model[2] %>% as.data.frame()

asian_lh_metrices <- asian_lh_model[2] %>% as.data.frame()

black_lp_metrices <- black_lp_model[2] %>% as.data.frame()

black_lh_metrices <- black_lh_model[2] %>% as.data.frame()

###### Save these files ######

write_rds(asian_lp_model, "/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/asian_lp_model.RDS")
write_rds(asian_lh_model, "/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/asian_lh_model.RDS")
write_rds(black_lp_model, "/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/black_lp_model.RDS")
write_rds(black_lh_model, "/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/black_lh_model.RDS")

```

```{r}

all_model <- bind_rows(mutate(asian_lp_metrices, group = "Asian_Americans", type = "Linked_progress"),
          mutate(asian_lh_metrices, group = "Asian_Americans", type = "Linked_hurt"),
          mutate(black_lp_metrices, group = "African_Americans", type = "Linked_progress"),
          mutate(black_lh_metrices, group = "African_Americans", type = "Linked_hurt"))

numerify <- function(data){
  data %>% unlist() %>% as.numeric()
}

irr_resummary <- data.frame(content_kappa = c(numerify(irr_summary[1:2, 2]), numerify(irr_summary[1:2, 3])),
                            content_agreement = c(numerify(irr_summary[1:2, 4]), numerify(irr_summary[1:2, 5])),
                            type = c("Linked_progress", "Linked_hurt", "Linked_progress", "Linked_hurt"),
                            group = c("Asian_Americans", "Asian_Americans", "African_Americans", "African_Americans")
)

merged_model <- all_model %>% right_join(irr_resummary) 

```

```{r}

accu_plot <- merged_model %>%
  gather(metrices, performance, acu, balanced_acu) %>%
  ggplot(aes(x = fct_reorder(model, performance), y = performance, fill = metrices)) +
    geom_col(position = "dodge") +
    geom_hline(aes(yintercept = content_agreement), linetype = "dashed") +
    facet_grid(type~group) +
    theme_base() +
    labs(title = "Accuracy", x = "Model", y = "Performance (%)") +
    coord_flip()

kappa_plot <- merged_model %>%
  ggplot(aes(x = fct_reorder(model, kappa), y = kappa)) +
    geom_col(position = "dodge") +
    geom_hline(aes(yintercept = content_kappa), linetype = "dashed") +
    facet_grid(type~group) +
    theme_base() +
    labs(title = "Reliability", x = "Model", y = "Performance (%)") +
    coord_flip()

ggarrange(accu_plot, kappa_plot, ncol = 1, nrow = 2)

ggsave("/home/jae/content-analysis-for-evaluating-ML-performances/outputs/ml_performance.png")

```

## 4. Applying to the unlabeled data

### 4.1. Building a prediction function 

```{r}

extract_feature <- function(dataset){
 
# Add doc ID (this var turns out to be very important for sorting)

dataset <- dataset %>%
  mutate(doc_id = row_number())

# Define stopwords

stopword <- tibble::enframe(stopwords::stopwords("en")) 
stopword <- rename(stopword, word=value) 

# Tokenize and reduce noisy tokens (=stopwords)

data_counts <- unnest_tokens(dataset, word, text) %>%
  anti_join(stopword, by = 'word') %>% # remove stopwords 
  count(doc_id, word, sort = TRUE)

words_10 <- data_counts %>%
  group_by(word) %>%
  summarise(n = n()) %>% 
  filter(n >= 10) %>%
  dplyr::select(word)

# TF-IDF

data_dtm <- data_counts %>%
  right_join(words_10, by = "word") %>%
  bind_tf_idf(word, doc_id, n) %>%
  cast_dtm(doc_id, word, tf_idf)

# Meta 

meta <- tibble(doc_id = as.numeric(dimnames(data_dtm)[[1]])) %>%
  left_join(dataset[!duplicated(dataset$doc_id), ], by = "doc_id") %>% as.data.frame()
 
matrix <- data_dtm %>% as.matrix() %>% as.data.frame() # Originally, it was a sparse matrix.

# response <- meta[, measure] %>% as.factor() # Originally, it was matrix.

return(matrix)
}

```

```{r}

unlabeled_asian$text <- clean_text(unlabeled_asian$text)
#unlabeled_black$text <- clean_text(unlabeled_black$text)

asian_dtm <- extract_feature(unlabeled_asian)
# black_dtm <- extract_feature(unlabeled_black)

asian_lp_preds <- predict(asian_lp_model[[1]][2], asian_dtm)

# asian_lh_preds <- predict(asian_lh_model[[1]][[2]], asian_dtm)

# black_lp_preds <- predict(black_lp_model[[1]][[2]], black_dtm)

# black_lh_preds <- predict(black_lh_model[[1]][[2]], black_dtm)

table(asian_lp_preds)
# table(asian_lh_preds)

```

## 5. Exporting results as a csv file 

```{r}

```

