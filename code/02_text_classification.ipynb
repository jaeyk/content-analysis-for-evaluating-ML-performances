{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries \n",
    "\n",
    "I adapted some Python code that my RAs developed for another project: https://github.com/jaeyk/ITS-Text-Classification/blob/master/code/05_classification.ipynb\n",
    "\n",
    "Import only relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/jae/anaconda3/lib/python3.6/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: six in /home/jae/.local/lib/python3.6/site-packages (from nltk) (1.13.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Numpy and Pandas \n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data visualization \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# REGEX and NLP\n",
    "\n",
    "import re\n",
    "import string\n",
    "!pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB # Naive-Bayes\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression # Linear models\n",
    "from xgboost import XGBClassifier # XG Boost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score # Accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score # Balanced accuracy score\n",
    "\n",
    "# Interface\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file path\n",
    "\n",
    "# root = tk.Tk()\n",
    "# root.withdraw()\n",
    "\n",
    "# file_path = filedialog.askopenfilename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The labeled data \n",
    "\n",
    "asian_sample = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_asian.csv\")\n",
    "black_sample = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_black.csv\")\n",
    "\n",
    "# The unlabeled data\n",
    "\n",
    "asian_unlabeled = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_asian.csv\")\n",
    "black_unlabeled = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_black.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>linked_progress</th>\n",
       "      <th>linked_hurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Lopez, Flora</td>\n",
       "      <td>1976-04-30</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nS.P.I.C.E. is a nutritional pr...</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976-09-30</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nCommunity control rather than ...</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976-06-30</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\"Peasants of the Second Fortre...</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chin, Doug</td>\n",
       "      <td>1976-10-31</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nMuch of what is now the Intern...</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chow, Ron</td>\n",
       "      <td>1976-02-29</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nInternational District Housing...</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        author        date                  source  \\\n",
       "0           1  Lopez, Flora  1976-04-30  International Examiner   \n",
       "1           2           NaN  1976-09-30  International Examiner   \n",
       "2           3           NaN  1976-06-30  International Examiner   \n",
       "3           4    Chin, Doug  1976-10-31  International Examiner   \n",
       "4           5     Chow, Ron  1976-02-29  International Examiner   \n",
       "\n",
       "                                                text  year  linked_progress  \\\n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\nS.P.I.C.E. is a nutritional pr...  1976                1   \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\nCommunity control rather than ...  1976                0   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\"Peasants of the Second Fortre...  1976                0   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\nMuch of what is now the Intern...  1976                0   \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\nInternational District Housing...  1976                1   \n",
       "\n",
       "   linked_hurt  \n",
       "0            0  \n",
       "1            0  \n",
       "2            1  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First five rows\n",
    "\n",
    "asian_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first column\n",
    "\n",
    "## Seen \n",
    "\n",
    "asian_sample = asian_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "black_sample = black_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "# An alternative way of doing this is asian_sample = asian_sample[['col1', 'col2']] \n",
    "\n",
    "## Unseen \n",
    "\n",
    "asian_unlabeled = asian_unlabeled.drop(['Unnamed: 0'], axis = 1)\n",
    "black_unlabeled = black_unlabeled.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert date column into datetime. This new data type allows us to extract some info from the column. For instance, `asian_samplep['date'].year` returns years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seen data \n",
    "asian_sample[\"date\"] = pd.to_datetime(asian_sample[\"date\"])\n",
    "black_sample[\"date\"] = pd.to_datetime(black_sample[\"date\"])\n",
    "\n",
    "# Unseen data \n",
    "asian_unlabeled[\"date\"] = pd.to_datetime(asian_unlabeled[\"date\"])\n",
    "black_unlabeled[\"date\"] = pd.to_datetime(black_unlabeled[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The N of labeled Asian American articles: 990 \n",
      "The N of labeled African American articles: 1008 \n",
      "The N of unlabeled Asian American articles: 7739 \n",
      "The N of unlabeled Asian American articles: 37743\n"
     ]
    }
   ],
   "source": [
    "# Check the size of each file\n",
    "\n",
    "print(\"The N of labeled Asian American articles:\", len(asian_sample['text']), \"\\n\"\n",
    "      \"The N of labeled African American articles:\", len(black_sample['text']), \"\\n\"\n",
    "      \"The N of unlabeled Asian American articles:\", len(asian_unlabeled['text']), \"\\n\"\n",
    "      \"The N of unlabeled Asian American articles:\", len(black_unlabeled['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of labeled Asian American articles was reduced as I remove 18 duplicates from the original sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters, punctuations, whitespace, and stopwords\n",
    "\n",
    "- I created a function for cleaning texts.\n",
    "- Removing stop words did not increase performance in this case. (So, I commented it out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_text(document):\n",
    "    document = document.str.lower() # lower case\n",
    "    document = document.str.replace('[\\r?\\n]','', regex = True)\n",
    "    document = document.str.replace('[^\\\\w\\\\s]','', regex = True)\n",
    "    document = document.str.replace('\\\\d+', '', regex = True)   \n",
    "    document = document.str.strip() # remove whitespace\n",
    "  #  document = document.apply(lambda x: \" \".join([y for y in x.split() if y not in stop_words]))\n",
    "    return(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works using one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friday nov  at  pm rev l s rubin pastor at oli...\n",
       "1    we have a large building an ante bellum buildi...\n",
       "2    ktvus televoters were back to being pretty upt...\n",
       "3    washington dc  washingtons appointed mayor wal...\n",
       "4    spokesmen for the congress of racial equality ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(black_sample['text']).head() # first 5 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seen\n",
    "\n",
    "asian_sample['text'] = clean_text(asian_sample['text'])\n",
    "black_sample['text'] = clean_text(black_sample['text'])\n",
    "\n",
    "# Unseen\n",
    "\n",
    "asian_unlabeled['text'] = clean_text(asian_unlabeled['text'])\n",
    "black_unlabeled['text'] = clean_text(black_unlabeled['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Here, we turn texts into a document-term matrix. These terms represent features in the model and we aim to find a combination of features that are most effective in predicting target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bag of Words (BOW)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features = 5000, # 5,000 is large enough\n",
    "    min_df = 1, # minimum frequency 1 \n",
    "    ngram_range = (1,2), # ngram \n",
    "    binary = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Document-term Matrix (DTM)\n",
    "\n",
    "def dtm_train(data, text, column, year):\n",
    "    \n",
    "    # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(data[text]).todense()   \n",
    "\n",
    "    # Response variable\n",
    "    \n",
    "    response = data[column].values # values \n",
    "\n",
    "    # Split into training and testing sets \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, response, \n",
    "                                                        test_size = 0.2, # training = 80%, test = 20%\n",
    "                                                        random_state = 1234, # for reproducibility\n",
    "                                                        stratify = data[year]) # stratifying by year\n",
    "    \n",
    "    # Label encode (normalize) response variable\n",
    "    \n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "    return(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Asian \n",
    "\n",
    "## Linked progress\n",
    "asian_lp_dtm = dtm_train(asian_sample, 'text', 'linked_progress', 'year')\n",
    "asian_X_train_lp = asian_lp_dtm[0]\n",
    "asian_y_train_lp = asian_lp_dtm[1]\n",
    "asian_X_test_lp = asian_lp_dtm[2]\n",
    "asian_y_test_lp = asian_lp_dtm[3]\n",
    "\n",
    "## Linked hurt \n",
    "asian_lh_dtm = dtm_train(asian_sample, 'text', 'linked_hurt', 'year')\n",
    "asian_X_train_lh = asian_lh_dtm[0]\n",
    "asian_y_train_lh = asian_lh_dtm[1]\n",
    "asian_X_test_lh = asian_lh_dtm[2]\n",
    "asian_y_test_lh = asian_lh_dtm[3]\n",
    "\n",
    "# Black\n",
    "\n",
    "## Linked progress\n",
    "black_lp_dtm = dtm_train(black_sample, 'text', 'linked_progress', 'year')\n",
    "black_X_train_lp = black_lp_dtm[0]\n",
    "black_y_train_lp = black_lp_dtm[1]\n",
    "black_X_test_lp = black_lp_dtm[2]\n",
    "black_y_test_lp = black_lp_dtm[3]\n",
    "\n",
    "## Linked hurt \n",
    "black_lh_dtm = dtm_train(black_sample, 'text', 'linked_hurt', 'year')\n",
    "black_X_train_lh = black_lh_dtm[0]\n",
    "black_y_train_lh = black_lh_dtm[1]\n",
    "black_X_test_lh = black_lh_dtm[2]\n",
    "black_y_test_lh = black_lh_dtm[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## fitting \n",
    "\n",
    "def fit_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(fit_intercept = True, penalty = 'l1', solver = 'saga') # Lasso\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def fit_bayes(X_train, y_train):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def fit_xgboost(X_train, y_train):\n",
    "    model = XGBClassifier(random_state = 42,\n",
    "                         seed = 2, \n",
    "                         colsample_bytree = 0.6,\n",
    "                         subsample = 0.7)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "## evaluating \n",
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy, \"\\n\"\n",
    "          \"Balanced accuracy:\", balanced_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asian_sample \n",
    "\n",
    "## linked progress\n",
    "asian_lp = fit_logistic_regression(asian_X_train_lp, asian_y_train_lp)\n",
    "asian_lp_bayes = fit_bayes(asian_X_train_lp, asian_y_train_lp)\n",
    "asian_lp_xgboost = fit_xgboost(asian_X_train_lp, asian_y_train_lp)\n",
    "\n",
    "## linked hurt \n",
    "asian_lh = fit_logistic_regression(asian_X_train_lh, asian_y_train_lh)\n",
    "asian_lh_bayes = fit_bayes(asian_X_train_lh, asian_y_train_lh)\n",
    "asian_lh_xgboost = fit_xgboost(asian_X_train_lh, asian_y_train_lh)\n",
    "\n",
    "# black_sample \n",
    "\n",
    "## linked progress\n",
    "black_lp = fit_logistic_regression(black_X_train_lp, black_y_train_lp)\n",
    "black_lp_bayes = fit_bayes(black_X_train_lp, black_y_train_lp)\n",
    "black_lp_xgboost = fit_xgboost(black_X_train_lp, black_y_train_lp)\n",
    "\n",
    "## linked hurt \n",
    "black_lh = fit_logistic_regression(black_X_train_lh, black_y_train_lh)\n",
    "black_lh_bayes = fit_bayes(black_X_train_lh, black_y_train_lh)\n",
    "black_lh_xgboost = fit_xgboost(black_X_train_lh, black_y_train_lh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asian linked progress: Logistic regression\n",
      "Accuracy: 0.7626262626262627 \n",
      "Balanced accuracy: 0.6352216748768473\n",
      "Asian linked progress: Naive Bayes\n",
      "Accuracy: 0.6363636363636364 \n",
      "Balanced accuracy: 0.5459359605911329\n",
      "Asian linked progress: XGBoost\n",
      "Accuracy: 0.7626262626262627 \n",
      "Balanced accuracy: 0.615024630541872\n",
      "Asian linked hurt: Logistic regression\n",
      "Accuracy: 0.8636363636363636 \n",
      "Balanced accuracy: 0.6732142857142858\n",
      "Asian linked hurt: Naive Bayes\n",
      "Accuracy: 0.8282828282828283 \n",
      "Balanced accuracy: 0.5291666666666667\n",
      "Asian linked hurt: XGBoost\n",
      "Accuracy: 0.8636363636363636 \n",
      "Balanced accuracy: 0.6321428571428571\n",
      "Black linked progress: Logistic regression\n",
      "Accuracy: 0.7772277227722773 \n",
      "Balanced accuracy: 0.6223214285714286\n",
      "Black linked progress: Naive Bayes\n",
      "Accuracy: 0.7326732673267327 \n",
      "Balanced accuracy: 0.6205357142857143\n",
      "Black linked progress: XGBoost\n",
      "Accuracy: 0.7970297029702971 \n",
      "Balanced accuracy: 0.5997023809523809\n",
      "Black linked hurt: Logistic regression\n",
      "Accuracy: 0.8465346534653465 \n",
      "Balanced accuracy: 0.6878086419753087\n",
      "Black linked hurt: Naive Bayes\n",
      "Accuracy: 0.801980198019802 \n",
      "Balanced accuracy: 0.5376543209876543\n",
      "Black linked hurt: XGBoost\n",
      "Accuracy: 0.8366336633663366 \n",
      "Balanced accuracy: 0.6439814814814815\n"
     ]
    }
   ],
   "source": [
    "# asian sample test \n",
    "\n",
    "print(\"Asian linked progress: Logistic regression\")\n",
    "test_model(asian_lp, asian_X_train_lp, asian_y_train_lp, asian_X_test_lp, asian_y_test_lp)\n",
    "print(\"Asian linked progress: Naive Bayes\")\n",
    "test_model(asian_lp_bayes, asian_X_train_lp, asian_y_train_lp, asian_X_test_lp, asian_y_test_lp)\n",
    "print(\"Asian linked progress: XGBoost\")\n",
    "test_model(asian_lp_xgboost, asian_X_train_lp, asian_y_train_lp, asian_X_test_lp, asian_y_test_lp)\n",
    "\n",
    "print(\"Asian linked hurt: Logistic regression\")\n",
    "test_model(asian_lh, asian_X_train_lh, asian_y_train_lh, asian_X_test_lh, asian_y_test_lh)\n",
    "print(\"Asian linked hurt: Naive Bayes\")\n",
    "test_model(asian_lh_bayes, asian_X_train_lh, asian_y_train_lh, asian_X_test_lh, asian_y_test_lh)\n",
    "print(\"Asian linked hurt: XGBoost\")\n",
    "test_model(asian_lh_xgboost, asian_X_train_lh, asian_y_train_lh, asian_X_test_lh, asian_y_test_lh)\n",
    "\n",
    "# black sample test\n",
    "\n",
    "print(\"Black linked progress: Logistic regression\")\n",
    "test_model(black_lp, black_X_train_lp, black_y_train_lp, black_X_test_lp, black_y_test_lp)\n",
    "print(\"Black linked progress: Naive Bayes\")\n",
    "test_model(black_lp_bayes, black_X_train_lp, black_y_train_lp, black_X_test_lp, black_y_test_lp)\n",
    "print(\"Black linked progress: XGBoost\")\n",
    "test_model(black_lp_xgboost, black_X_train_lp, black_y_train_lp, black_X_test_lp, black_y_test_lp)\n",
    "\n",
    "print(\"Black linked hurt: Logistic regression\")\n",
    "test_model(black_lh, black_X_train_lh, black_y_train_lh, black_X_test_lh, black_y_test_lh)\n",
    "print(\"Black linked hurt: Naive Bayes\")\n",
    "test_model(black_lh_bayes, black_X_train_lh, black_y_train_lh, black_X_test_lh, black_y_test_lh)\n",
    "print(\"Black linked hurt: XGBoost\")\n",
    "test_model(black_lh_xgboost, black_X_train_lh, black_y_train_lh, black_X_test_lh, black_y_test_lh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_text(text, model):   \n",
    "      \n",
    "     # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(text).todense()\n",
    "    \n",
    "    preds = model.predict(features)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asian\n",
    "## unlabeled\n",
    "asian_lp_full = test_text(asian_unlabeled['text'], asian_lp)\n",
    "asian_lh_full = test_text(asian_unlabeled['text'], asian_lh)\n",
    "\n",
    "# black\n",
    "## unlabeled \n",
    "black_lp_full = test_text(black_unlabeled['text'], black_lp)\n",
    "black_lh_full = test_text(black_unlabeled['text'], black_lh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asian linked progress: 254 asian linked hurt: 147 black linked progress: 257 blakc linked hurt: 196\n"
     ]
    }
   ],
   "source": [
    "# The original \n",
    "\n",
    "print(\"asian linked progress:\", sum(asian_sample['linked_progress']),\n",
    "      \"asian linked hurt:\", sum(asian_sample['linked_hurt']),\n",
    "      \"black linked progress:\", sum(black_sample['linked_progress']),\n",
    "      \"blakc linked hurt:\", sum(black_sample['linked_hurt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asian linked progress: 220 asian linked hurt: 138 black linked progress: 246 blakc linked hurt: 174\n"
     ]
    }
   ],
   "source": [
    "# The machine coded\n",
    "\n",
    "print(\"asian linked progress:\", sum(test_text(asian_sample['text'], asian_lp)),\n",
    "      \"asian linked hurt:\", sum(test_text(asian_sample['text'], asian_lh)),\n",
    "      \"black linked progress:\", sum(test_text(black_sample['text'], black_lp)),\n",
    "      \"blakc linked hurt:\", sum(test_text(black_sample['text'], black_lh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asian linked progress: 2369 asian linked hurt: 2976 black linked progress: 15087 black linked hurt: 24212\n",
      "asian progress / hurt 0.8 black progress /un hurt 0.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"asian linked progress:\", sum(asian_lp_full),\n",
    "      \"asian linked hurt:\", sum(asian_lh_full),\n",
    "      \"black linked progress:\", sum(black_lp_full),\n",
    "      \"black linked hurt:\", sum(black_lh_full))\n",
    "\n",
    "print(\"asian progress / hurt\", round(sum(asian_lp_full)/sum(asian_lh_full),2),\n",
    "      \"black progress /un hurt\", round(sum(black_lp_full)/sum(black_lh_full),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results as csv files \n",
    "\n",
    "I saved the resutls as csv files to plot them in R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Asian\n",
    "#asian_lp_data = pd.DataFrame(asian_lp_full).rename(columns = {0:'labeled_linked_progress'})\n",
    "#asian_lp_data.to_csv(\"/home/jae/linked_fate_evolution/Output/asian_lp_data.csv\")\n",
    "\n",
    "#asian_lh_data = pd.DataFrame(asian_lh_full).rename(columns = {0:'labeled_linked_hurt'})\n",
    "#asian_lh_data.to_csv(\"/home/jae/linked_fate_evolution/Output/asian_lh_data.csv\")\n",
    "\n",
    "# Black\n",
    "#black_lp_data = pd.DataFrame(black_lp_full).rename(columns = {0:'labeled_linked_progress'})\n",
    "#black_lp_data.to_csv(\"/home/jae/linked_fate_evolution/Output/black_lp_data.csv\")\n",
    "\n",
    "#black_lh_data = pd.DataFrame(black_lh_full).rename(columns = {0:'labeled_linked_hurt'})\n",
    "#black_lh_data.to_csv(\"/home/jae/linked_fate_evolution/Output/black_lh_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
