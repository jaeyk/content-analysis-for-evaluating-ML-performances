{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries \n",
    "\n",
    "I adapted some Python code that my RAs developed for another project: https://github.com/jaeyk/ITS-Text-Classification/blob/master/code/05_classification.ipynb\n",
    "\n",
    "Import only relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/jae/anaconda3/lib/python3.6/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: six in /home/jae/.local/lib/python3.6/site-packages (from nltk) (1.13.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Numpy and Pandas \n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data visualization \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# REGEX and NLP\n",
    "\n",
    "import re\n",
    "import string\n",
    "!pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB # Naive-Bayes\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression # Linear models\n",
    "from xgboost import XGBClassifier # XG Boost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score # Accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score # Balanced accuracy score\n",
    "from sklearn.metrics import cohen_kappa_score # Cohen's Kappa score\n",
    "from sklearn.utils import resample # for resampling\n",
    "\n",
    "# Interface\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file path\n",
    "\n",
    "# root = tk.Tk()\n",
    "# root.withdraw()\n",
    "\n",
    "# file_path = filedialog.askopenfilename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The labeled data \n",
    "\n",
    "asian_sample = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_asian.csv\")\n",
    "black_sample = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/sample_black.csv\")\n",
    "\n",
    "# The unlabeled data\n",
    "\n",
    "asian_unlabeled = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_asian.csv\")\n",
    "black_unlabeled = pd.read_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/unlabeled_black.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>linked_progress</th>\n",
       "      <th>linked_hurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Lopez, Flora</td>\n",
       "      <td>1976-04-30</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nS.P.I.C.E. is a nutritional pr...</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976-09-30</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nCommunity control rather than ...</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976-06-30</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\"Peasants of the Second Fortre...</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chin, Doug</td>\n",
       "      <td>1976-10-31</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nMuch of what is now the Intern...</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chow, Ron</td>\n",
       "      <td>1976-02-29</td>\n",
       "      <td>International Examiner</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nInternational District Housing...</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        author        date                  source  \\\n",
       "0           1  Lopez, Flora  1976-04-30  International Examiner   \n",
       "1           2           NaN  1976-09-30  International Examiner   \n",
       "2           3           NaN  1976-06-30  International Examiner   \n",
       "3           4    Chin, Doug  1976-10-31  International Examiner   \n",
       "4           5     Chow, Ron  1976-02-29  International Examiner   \n",
       "\n",
       "                                                text  year  linked_progress  \\\n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\nS.P.I.C.E. is a nutritional pr...  1976                1   \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\nCommunity control rather than ...  1976                0   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\"Peasants of the Second Fortre...  1976                0   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\nMuch of what is now the Intern...  1976                0   \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\nInternational District Housing...  1976                1   \n",
       "\n",
       "   linked_hurt  \n",
       "0            0  \n",
       "1            0  \n",
       "2            1  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First five rows\n",
    "\n",
    "asian_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first column\n",
    "\n",
    "## Seen \n",
    "\n",
    "asian_sample = asian_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "black_sample = black_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "# An alternative way of doing this is asian_sample = asian_sample[['col1', 'col2']] \n",
    "\n",
    "## Unseen \n",
    "\n",
    "asian_unlabeled = asian_unlabeled.drop(['Unnamed: 0'], axis = 1)\n",
    "black_unlabeled = black_unlabeled.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert date column into datetime. This new data type allows us to extract some info from the column. For instance, `asian_samplep['date'].year` returns years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seen data \n",
    "asian_sample[\"date\"] = pd.to_datetime(asian_sample[\"date\"])\n",
    "black_sample[\"date\"] = pd.to_datetime(black_sample[\"date\"])\n",
    "\n",
    "# Unseen data \n",
    "asian_unlabeled[\"date\"] = pd.to_datetime(asian_unlabeled[\"date\"])\n",
    "black_unlabeled[\"date\"] = pd.to_datetime(black_unlabeled[\"date\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the balance of target values: **imbalanced**. I used a resampling method (upsampling/oversampling) to address this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    736\n",
       "1    254\n",
       "Name: linked_progress, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the balance of target values \n",
    "\n",
    "asian_sample['linked_progress'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    843\n",
       "1    147\n",
       "Name: linked_hurt, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asian_sample['linked_hurt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    751\n",
       "1    257\n",
       "Name: linked_progress, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_sample['linked_progress'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    812\n",
       "1    196\n",
       "Name: linked_hurt, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_sample['linked_hurt'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of labeled Asian American articles was reduced as I remove 18 duplicates from the original sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters, punctuations, whitespace, and stopwords\n",
    "\n",
    "- I created a function for cleaning texts.\n",
    "- Removing stop words did not increase performance in this case. (So, I commented it out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_text(document):\n",
    "    document = document.str.lower() # lower case\n",
    "    document = document.str.replace('[\\r?\\n]','', regex = True)\n",
    "    document = document.str.replace('[^\\\\w\\\\s]','', regex = True)\n",
    "    document = document.str.replace('\\\\d+', '', regex = True)   \n",
    "    document = document.str.strip() # remove whitespace\n",
    "  #  document = document.apply(lambda x: \" \".join([y for y in x.split() if y not in stop_words]))\n",
    "    return(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works using one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friday nov  at  pm rev l s rubin pastor at oli...\n",
       "1    we have a large building an ante bellum buildi...\n",
       "2    ktvus televoters were back to being pretty upt...\n",
       "3    washington dc  washingtons appointed mayor wal...\n",
       "4    spokesmen for the congress of racial equality ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(black_sample['text']).head() # first 5 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seen\n",
    "\n",
    "asian_sample['text'] = clean_text(asian_sample['text'])\n",
    "black_sample['text'] = clean_text(black_sample['text'])\n",
    "\n",
    "# Unseen\n",
    "\n",
    "asian_unlabeled['text'] = clean_text(asian_unlabeled['text'])\n",
    "black_unlabeled['text'] = clean_text(black_unlabeled['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Here, we turn texts into a document-term matrix. These terms represent features in the model and we aim to find a combination of features that are most effective in predicting target values.\n",
    "\n",
    "### Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bag of Words (BOW)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features = 5000, # 5,000 is large enough\n",
    "    min_df = 1, # minimum frequency 1 \n",
    "    ngram_range = (1,2), # ngram \n",
    "    binary = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of things happened here. \n",
    "\n",
    "- Resampling to correct the imbalanced classes: `upsampled` the minority class \n",
    "- Converting text into a `document-term matrix` \n",
    "- Splitting the matrix into the training and testing set using `stratified random sampling`\n",
    "\n",
    "### Resampling, Creating DTM, and Splitting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dtm_train_resample(data, text, column, year):\n",
    "    \n",
    "    ############################### RESAMPLING ################################\n",
    "    \n",
    "    # Split into majority and minority classes: # I adapted some code from here: https://elitedatascience.com/imbalanced-classes \n",
    "       \n",
    "    df_majority = data[data[column] == 0]\n",
    "    df_minority = data[data[column] == 1]\n",
    "    \n",
    "    # Upsample (oversample) minority class \n",
    "    \n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                 replace = True,     # sample with replacement\n",
    "                                 n_samples = 750,    # to match majority class\n",
    "                                 random_state = 1234) # reproducible results\n",
    "    \n",
    "    # Combine majority class with upsampled minority class\n",
    "    data = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    ############################### DOCUMENT-TERM MATRIX ################################\n",
    "    \n",
    "    # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(data[text]).todense() # Turn into a sparse matrix    \n",
    "\n",
    "    # Response variable\n",
    "    \n",
    "    response = data[column].values # values \n",
    "\n",
    "    ############################### STRATIFIED RANDOM SAMPLING ################################\n",
    "    \n",
    "    # Split into training and testing sets \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, response, \n",
    "                                                        test_size = 0.2, # training = 80%, test = 20%\n",
    "                                                        random_state = 1234, # for reproducibility\n",
    "                                                        stratify = data[year]) # stratifying by year\n",
    "    \n",
    "    # Label encode (normalize) response variable\n",
    "    \n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "    return(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training and testing data and response variables\n",
    "\n",
    "I created training and testing data (text features) and their response variables using the custom function shown above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Asian American newspapers \n",
    "\n",
    "## Linked progress\n",
    "asian_lp_dtm = dtm_train_resample(asian_sample, 'text', 'linked_progress', 'year')\n",
    "asian_X_train_lp = asian_lp_dtm[0]\n",
    "asian_y_train_lp = asian_lp_dtm[1]\n",
    "asian_X_test_lp = asian_lp_dtm[2]\n",
    "asian_y_test_lp = asian_lp_dtm[3]\n",
    "\n",
    "## Linked hurt \n",
    "asian_lh_dtm = dtm_train_resample(asian_sample, 'text', 'linked_hurt', 'year')\n",
    "asian_X_train_lh = asian_lh_dtm[0]\n",
    "asian_y_train_lh = asian_lh_dtm[1]\n",
    "asian_X_test_lh = asian_lh_dtm[2]\n",
    "asian_y_test_lh = asian_lh_dtm[3]\n",
    "\n",
    "# African American newspapers\n",
    "\n",
    "## Linked progress\n",
    "black_lp_dtm = dtm_train_resample(black_sample, 'text', 'linked_progress', 'year')\n",
    "black_X_train_lp = black_lp_dtm[0]\n",
    "black_y_train_lp = black_lp_dtm[1]\n",
    "black_X_test_lp = black_lp_dtm[2]\n",
    "black_y_test_lp = black_lp_dtm[3]\n",
    "\n",
    "## Linked hurt \n",
    "black_lh_dtm = dtm_train_resample(black_sample, 'text', 'linked_hurt', 'year')\n",
    "black_X_train_lh = black_lh_dtm[0]\n",
    "black_y_train_lh = black_lh_dtm[1]\n",
    "black_X_test_lh = black_lh_dtm[2]\n",
    "black_y_test_lh = black_lh_dtm[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate a ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "\n",
    "def fit_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(fit_intercept = True, penalty = 'l1', solver = 'saga') # Lasso\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Naive-Bayes \n",
    "\n",
    "def fit_bayes(X_train, y_train):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# XG Boost \n",
    "\n",
    "def fit_xgboost(X_train, y_train):\n",
    "    model = XGBClassifier(random_state = 42,\n",
    "                         seed = 2, \n",
    "                         colsample_bytree = 0.6,\n",
    "                         subsample = 0.7)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for evaluating ML models (accuracy, balanced accuracy, and Cohen's kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy, \"\\n\"\n",
    "          \"Balanced accuracy:\", balanced_accuracy, \"\\n\"\n",
    "          \"Cohen's Kappa:\", kappa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asian American newspapers\n",
    "\n",
    "## Linked progress\n",
    "asian_lp = fit_logistic_regression(asian_X_train_lp, asian_y_train_lp)\n",
    "asian_lp_bayes = fit_bayes(asian_X_train_lp, asian_y_train_lp)\n",
    "asian_lp_xgboost = fit_xgboost(asian_X_train_lp, asian_y_train_lp)\n",
    "\n",
    "## Linked hurt \n",
    "asian_lh = fit_logistic_regression(asian_X_train_lh, asian_y_train_lh)\n",
    "asian_lh_bayes = fit_bayes(asian_X_train_lh, asian_y_train_lh)\n",
    "asian_lh_xgboost = fit_xgboost(asian_X_train_lh, asian_y_train_lh)\n",
    "\n",
    "# African American newspapers\n",
    "\n",
    "## Linked progress\n",
    "black_lp = fit_logistic_regression(black_X_train_lp, black_y_train_lp)\n",
    "black_lp_bayes = fit_bayes(black_X_train_lp, black_y_train_lp)\n",
    "black_lp_xgboost = fit_xgboost(black_X_train_lp, black_y_train_lp)\n",
    "\n",
    "## Linked hurt \n",
    "black_lh = fit_logistic_regression(black_X_train_lh, black_y_train_lh)\n",
    "black_lh_bayes = fit_bayes(black_X_train_lh, black_y_train_lh)\n",
    "black_lh_xgboost = fit_xgboost(black_X_train_lh, black_y_train_lh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asian linked progress: Logistic regression\n",
      "Accuracy: 0.9194630872483222 \n",
      "Balanced accuracy: 0.9189980628012795 \n",
      "Cohen's Kappa: 0.8387518600351715\n",
      "Asian linked progress: Naive Bayes\n",
      "Accuracy: 0.8221476510067114 \n",
      "Balanced accuracy: 0.8223408568725503 \n",
      "Cohen's Kappa: 0.6443914081145584\n",
      "Asian linked progress: XGBoost\n",
      "Accuracy: 0.87248322147651 \n",
      "Balanced accuracy: 0.8720998333108079 \n",
      "Cohen's Kappa: 0.7447364861818674\n",
      "Asian linked hurt: Logistic regression\n",
      "Accuracy: 0.9561128526645768 \n",
      "Balanced accuracy: 0.9595375722543353 \n",
      "Cohen's Kappa: 0.912248988092899\n",
      "Asian linked hurt: Naive Bayes\n",
      "Accuracy: 0.9059561128526645 \n",
      "Balanced accuracy: 0.9052775358302321 \n",
      "Cohen's Kappa: 0.810555071660464\n",
      "Asian linked hurt: XGBoost\n",
      "Accuracy: 0.9592476489028213 \n",
      "Balanced accuracy: 0.961358777417056 \n",
      "Cohen's Kappa: 0.9183002029196793\n",
      "Black linked progress: Logistic regression\n",
      "Accuracy: 0.8637873754152824 \n",
      "Balanced accuracy: 0.8637086092715232 \n",
      "Cohen's Kappa: 0.7275296403417747\n",
      "Black linked progress: Naive Bayes\n",
      "Accuracy: 0.8272425249169435 \n",
      "Balanced accuracy: 0.8270860927152318 \n",
      "Cohen's Kappa: 0.6543744203506603\n",
      "Black linked progress: XGBoost\n",
      "Accuracy: 0.8272425249169435 \n",
      "Balanced accuracy: 0.8272185430463577 \n",
      "Cohen's Kappa: 0.6544659808380061\n",
      "Black linked hurt: Logistic regression\n",
      "Accuracy: 0.939297124600639 \n",
      "Balanced accuracy: 0.9409083476412394 \n",
      "Cohen's Kappa: 0.8788526961233678\n",
      "Black linked hurt: Naive Bayes\n",
      "Accuracy: 0.865814696485623 \n",
      "Balanced accuracy: 0.8676723080696591 \n",
      "Cohen's Kappa: 0.7323806033956273\n",
      "Black linked hurt: XGBoost\n",
      "Accuracy: 0.9297124600638977 \n",
      "Balanced accuracy: 0.9305248957566838 \n",
      "Cohen's Kappa: 0.8595037949889823\n"
     ]
    }
   ],
   "source": [
    "# Asian American newspapers\n",
    "\n",
    "print(\"Asian linked progress: Logistic regression\")\n",
    "test_model(asian_lp, asian_X_train_lp, asian_y_train_lp, asian_X_test_lp, asian_y_test_lp)\n",
    "print(\"Asian linked progress: Naive Bayes\")\n",
    "test_model(asian_lp_bayes, asian_X_train_lp, asian_y_train_lp, asian_X_test_lp, asian_y_test_lp)\n",
    "print(\"Asian linked progress: XGBoost\")\n",
    "test_model(asian_lp_xgboost, asian_X_train_lp, asian_y_train_lp, asian_X_test_lp, asian_y_test_lp)\n",
    "\n",
    "print(\"Asian linked hurt: Logistic regression\")\n",
    "test_model(asian_lh, asian_X_train_lh, asian_y_train_lh, asian_X_test_lh, asian_y_test_lh)\n",
    "print(\"Asian linked hurt: Naive Bayes\")\n",
    "test_model(asian_lh_bayes, asian_X_train_lh, asian_y_train_lh, asian_X_test_lh, asian_y_test_lh)\n",
    "print(\"Asian linked hurt: XGBoost\")\n",
    "test_model(asian_lh_xgboost, asian_X_train_lh, asian_y_train_lh, asian_X_test_lh, asian_y_test_lh)\n",
    "\n",
    "# African American newspapers\n",
    "\n",
    "print(\"Black linked progress: Logistic regression\")\n",
    "test_model(black_lp, black_X_train_lp, black_y_train_lp, black_X_test_lp, black_y_test_lp)\n",
    "print(\"Black linked progress: Naive Bayes\")\n",
    "test_model(black_lp_bayes, black_X_train_lp, black_y_train_lp, black_X_test_lp, black_y_test_lp)\n",
    "print(\"Black linked progress: XGBoost\")\n",
    "test_model(black_lp_xgboost, black_X_train_lp, black_y_train_lp, black_X_test_lp, black_y_test_lp)\n",
    "\n",
    "print(\"Black linked hurt: Logistic regression\")\n",
    "test_model(black_lh, black_X_train_lh, black_y_train_lh, black_X_test_lh, black_y_test_lh)\n",
    "print(\"Black linked hurt: Naive Bayes\")\n",
    "test_model(black_lh_bayes, black_X_train_lh, black_y_train_lh, black_X_test_lh, black_y_test_lh)\n",
    "print(\"Black linked hurt: XGBoost\")\n",
    "test_model(black_lh_xgboost, black_X_train_lh, black_y_train_lh, black_X_test_lh, black_y_test_lh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "### Function for predicting the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_text(text, model):   \n",
    "      \n",
    "    # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(text).todense()\n",
    "    \n",
    "    # Prediction\n",
    "    \n",
    "    preds = model.predict(features)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asian Americans \n",
    "\n",
    "asian_lp_full = test_text(asian_unlabeled['text'], asian_lp)\n",
    "asian_lh_full = test_text(asian_unlabeled['text'], asian_lh)\n",
    "\n",
    "# African Americans \n",
    "\n",
    "black_lp_full = test_text(black_unlabeled['text'], black_lp)\n",
    "black_lh_full = test_text(black_unlabeled['text'], black_lh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export classification results as CSV files \n",
    "\n",
    "I saved the classification results as CSV files to plot them in R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename new columns  \n",
    "\n",
    "asian_lp_data = pd.DataFrame(asian_lp_full).rename(columns = {0:'labeled_linked_progress'})\n",
    "asian_lh_data = pd.DataFrame(asian_lh_full).rename(columns = {0:'labeled_linked_hurt'})\n",
    "black_lp_data = pd.DataFrame(black_lp_full).rename(columns = {0:'labeled_linked_progress'})\n",
    "black_lh_data = pd.DataFrame(black_lh_full).rename(columns = {0:'labeled_linked_hurt'})\n",
    "\n",
    "# Save data \n",
    "\n",
    "asian_lp_data.to_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/asian_lp_data.csv\")\n",
    "asian_lh_data.to_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/asian_lh_data.csv\")\n",
    "black_lp_data.to_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/black_lp_data.csv\")\n",
    "black_lh_data.to_csv(\"/home/jae/content-analysis-for-evaluating-ML-performances/processed_data/black_lh_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the final data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labeled_linked_progress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labeled_linked_progress\n",
       "0                        1\n",
       "1                        1\n",
       "2                        0\n",
       "3                        0\n",
       "4                        1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asian_lp_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
