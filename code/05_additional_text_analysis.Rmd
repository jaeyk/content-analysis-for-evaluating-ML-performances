---
html_document:
  toc: True
  theme: united
  number_sections: True
---

# Setup

```{r include=FALSE}

pacman::p_load(
        tidyverse, # for the tidyverse framework
        ggpubr, # for arranging ggplots
        ggthemes, # for fancy ggplot themes
        ggrepel, # annotating text in ggplot2 
        tidytext, # for tidytext
        furrr, # for multiprocessing
        patchwork, # for arranging images
        scales, # for scales 
        here) # reproducibility 

# devtools::install_github("jaeyk/makereproducible")
library(makereproducible)

# Import R scripts

script_list <- list.files(paste0(here::here(), "/functions"),
  pattern = "*.r|*.R",
  full.names = TRUE
)


for (i in 1:length(script_list))
{
  source(script_list[[i]])
}

# for publication-friendly theme
theme_set(theme_pubr())
```


# Import files

```{r}
# Maximum threshold 
full_articles <- read_csv(here("processed_data", "full_articles.csv"))

# Minimum threshold 
full_articles_gran <- read_csv(here("processed_data", "full_articles_gran.csv"))

full_articles_gran <- full_articles_gran %>%
  mutate(lp_exclusive = ifelse(linked_progress == 1 & linked_hurt == 0, 1, 0),
         lh_exclusive = ifelse(linked_progress == 0 & linked_hurt == 1, 1, 0),
         lf_mixed = ifelse(linked_progress == 1 & linked_hurt == 1, 1, 0))
```

# Relative most frequent words

```{r tidy}
# Tidy text 
tidy_articles <- tidy_text(full_articles, lp_exclusive, lh_exclusive)

tidy_articles_gran <- tidy_text(full_articles_gran, lp_exclusive, lh_exclusive)

write_csv(tidy_articles, here("processed_data", "tidy_articles.csv"))

write_csv(tidy_articles_gran, here("processed_data", "tidy_articles_gran.csv"))

```

```{r tokenize}
tidy_articles <- data.table::fread(here("processed_data", "tidy_articles.csv"))

tidy_articles_gran <- data.table::fread(here("processed_data", "tidy_articles_gran.csv"))

# Tokenize text 

tokenized_articles <- tokenize_text(tidy_articles)

tokenized_articles_gran <- tokenize_text(tidy_articles_gran)

```

```{r count frequency}

# Count word frequency 

wf <- create_word_frequency(tokenized_articles)

wf_gran <- create_word_frequency(tokenized_articles_gran)

write_csv(wf, here("processed_data", "word_frequency.csv"))

write_csv(wf_gran, here("processed_data", "word_frequency_gran.csv"))

wf <- data.table::fread(here("processed_data", "word_frequency.csv"))

wf_gran <- data.table::fread(here("processed_data", "word_frequency_gran.csv"))

```

```{r filter}

filter_wf <- filter_words(wf) %>% filter_n() 

filter_wf_gran <- filter_words(wf_gran) %>% filter_n()

```


```{r visuazlie}

visualize_wf(filter_wf, "Maximum threshold") +

ggsave(here("outputs", "relative_word_freq.png"))

visualize_wf(filter_wf_gran, "Minimum threshold")

ggsave(here("outputs", "relative_word_freq_gran.png"),
       height = 7)

```